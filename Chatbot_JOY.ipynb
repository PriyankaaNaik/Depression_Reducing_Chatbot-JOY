{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final_Chatbot_JOY.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIOAOoU-LlG7",
        "outputId": "fd45b333-5f72-4b08-fe94-f626bdde69ea"
      },
      "source": [
        "pip install tflearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 21.0 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 26.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 51 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 61 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 92 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 102 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 107 kB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=f8bda6dc600df9e7a01de089f05ea90058c72b66812853964f8cc1b41d214076\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/14/2e/1d8e28cc47a5a931a2fb82438c9e37ef9246cc6a3774520271\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbf0LbBxPRaB",
        "outputId": "6df8cac0-c422-4d7d-937e-95631222e96c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "import numpy\n",
        "import tflearn\n",
        "import tensorflow\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDOKRKLWPOOh"
      },
      "source": [
        "with open(\"intents.json\") as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "try:\n",
        "    with open(\"data.pickle\", \"rb\") as f:\n",
        "        words, labels, training, output = pickle.load(f)\n",
        "except:\n",
        "    words = []\n",
        "    labels = []\n",
        "    docs_x = []\n",
        "    docs_y = []\n",
        "\n",
        "    for intent in data[\"intents\"]:\n",
        "        for pattern in intent[\"patterns\"]:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            docs_x.append(wrds)\n",
        "            docs_y.append(intent[\"tag\"])\n",
        "\n",
        "        if intent[\"tag\"] not in labels:\n",
        "            labels.append(intent[\"tag\"])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    labels = sorted(labels)\n",
        "\n",
        "    training = []\n",
        "    output = []\n",
        "\n",
        "    out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "    for x, doc in enumerate(docs_x):\n",
        "        bag = []\n",
        "\n",
        "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "\n",
        "        for w in words:\n",
        "            if w in wrds:\n",
        "                bag.append(1)\n",
        "            else:\n",
        "                bag.append(0)\n",
        "\n",
        "        output_row = out_empty[:]\n",
        "        output_row[labels.index(docs_y[x])] = 1\n",
        "\n",
        "        training.append(bag)\n",
        "        output.append(output_row)\n",
        "\n",
        "\n",
        "    training = numpy.array(training)\n",
        "    output = numpy.array(output)\n",
        "\n",
        "    with open(\"data.pickle\", \"wb\") as f:\n",
        "        pickle.dump((words, labels, training, output), f)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhy_DNUN18p"
      },
      "source": [
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "            \n",
        "    return numpy.array(bag)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aS3kUNwKiVr",
        "outputId": "7d09f51a-a900-4821-937b-f3c354a12b06"
      },
      "source": [
        "tensorflow.compat.v1.reset_default_graph()\n",
        "\n",
        "net = tflearn.input_data(shape=[None, len(training[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "model = tflearn.DNN(net)\n",
        "\n",
        "model.fit(training, output, n_epoch=1500, batch_size=8, show_metric=True)\n",
        "model.save(\"model.tflearn\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 6599  | total loss: \u001b[1m\u001b[32m0.11051\u001b[0m\u001b[0m | time: 0.165s\n",
            "| Adam | epoch: 200 | loss: 0.11051 - acc: 0.9756 -- iter: 256/263\n",
            "Training Step: 6600  | total loss: \u001b[1m\u001b[32m0.10152\u001b[0m\u001b[0m | time: 0.175s\n",
            "| Adam | epoch: 200 | loss: 0.10152 - acc: 0.9780 -- iter: 263/263\n",
            "--\n",
            "INFO:tensorflow:/content/model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcimpsHhJ5bB",
        "outputId": "d483f323-aa4b-467f-d752-d2af6cd2eb56"
      },
      "source": [
        "user_name = input()\n",
        "bot_name = \"JOY\"\n",
        "bot_template =  bot_name + \" : \"\n",
        "user_template = user_name + \" : \""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Priyanka\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSqIRFQYQCfu",
        "outputId": "64030249-608c-4ea4-a916-a1fa08759714"
      },
      "source": [
        "def chat():\n",
        "  print(\"Hi, this is JOY, nice to see you\")\n",
        "  while True:\n",
        "    inp = input(user_template)\n",
        "    if inp.capitalize() == \"Stop\":\n",
        "      break\n",
        "\n",
        "    results = model.predict([bag_of_words(inp, words)])\n",
        "    results_index = numpy.argmax(results)\n",
        "    tag = labels[results_index]\n",
        "      \n",
        "    for tg in data[\"intents\"]:\n",
        "      if tg['tag'] == tag:\n",
        "        responses = tg['responses']\n",
        "    \n",
        "    print(random.choice(responses))\n",
        "   \n",
        "\n",
        "chat()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi, this is JOY, nice to see you\n",
            "Pri : hi\n",
            "Hey\n",
            "Pri : How are you\n",
            "I am good, thank you\n",
            "Pri : I am sad\n",
            "Can't see you like this, please tell me what's wrong?\n",
            "Pri : My parents don't love me\n",
            "Hey, parents are after all parents. They just pretend to hate us but the truth is, no one can love us like them. Time will heal everything. Stay positive\n",
            "Pri : I am broken\n",
            "Hey, calm down, I don't know why it happened but I just know that everything happens for a reason, everything will be fine soon\n",
            "Pri : Thank you\n",
            "Always!\n",
            "Pri : I am bad\n",
            "Not at all dear. I think you are very sensitive. And sensitive people get hurt very often and that doesn't make you bad or wrong. Trust yourself. You are the best.\n",
            "Pri : stop\n"
          ]
        }
      ]
    }
  ]
}